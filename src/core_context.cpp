/*
 *
 *  Managed Data Structures
 *  Copyright Â© 2016 Hewlett Packard Enterprise Development Company LP.
 *
 *  This program is free software: you can redistribute it and/or modify
 *  it under the terms of the GNU Lesser General Public License as published by
 *  the Free Software Foundation, either version 3 of the License, or
 *  (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU Lesser General Public License for more details.
 *
 *  You should have received a copy of the GNU Lesser General Public License
 *  along with this program.  If not, see <http://www.gnu.org/licenses/>.
 *
 *  As an exception, the copyright holders of this Library grant you permission
 *  to (i) compile an Application with the Library, and (ii) distribute the 
 *  Application containing code generated by the Library and added to the 
 *  Application during this compilation process under terms of your choice, 
 *  provided you also meet the terms and conditions of the Application license.
 *
 */

/*
 * core_context.cpp
 *
 *  Created on: Nov 13, 2014
 *      Author: evank
 */

#include <unordered_map>
#include <unordered_set>
#include <queue>
#include "core/core_context.h"
#include "core/core_msv.h"
#include "mpgc/gc_vector.h"

namespace mds {
  namespace core {

    constexpr iso_context::global_context_t iso_context::global;

    class iso_context::unpublished_state : public iso_context::state_t {
    public:

      static constexpr discriminator_type discrim = state_types::UNPUBLISHED;
    private:
      friend class iso_context;
      const conflict_list _conflicts;
      const modified_vc_list _modified_vcs;
      const blocking_mod_list _blockers;

      static conflict_list prune(const conflict_list &list) {
        return list;
      }

      static modified_vc_list prune(const modified_vc_list &list) {
        return list.skip_satisfying([](const auto &m) {
            return m->cleared();
          });
      }

      static blocking_mod_list prune(const blocking_mod_list &list) {
        return list.skip_satisfying([](const auto &b) {
            return !b->pending();
          });
      }

      class drop_conflicts_ctor {};

      class roll_forward_ctor {};

    public:

      unpublished_state(gc_token &gc,
                        const gc_ptr<const published_state> &prior,
                        const gc_ptr<const conflict> &c,
                        discriminator_type d = discrim)
        : state_t{gc, prior, d},
          _conflicts{c}
      {}
      unpublished_state(gc_token &gc,
                        const gc_ptr<const published_state> &prior,
                        const gc_ptr<modified_value_chain> &modified_vc,
                        discriminator_type d = discrim)
        : state_t{gc, prior, d},
          _modified_vcs{modified_vc}
      {}
      unpublished_state(gc_token &gc,
                        const gc_ptr<const published_state> &prior,
                        const gc_ptr<blocking_mod> &b,
                        discriminator_type d = discrim)
        : state_t{gc, prior, d},
          _blockers{b}
      {}
      unpublished_state(gc_token &gc,
                        const unpublished_state &old,
                        gc_ptr<const conflict> c,
                        discriminator_type d = discrim)
        : state_t{gc, old.prior_published_state, d},
          _conflicts{c, prune(old._conflicts)},
          _modified_vcs{prune(old._modified_vcs)},
          _blockers{prune(old._blockers)}
      {}
      unpublished_state(gc_token &gc,
                        const unpublished_state &old,
                        gc_ptr<modified_value_chain> modified_vcs,
                        discriminator_type d = discrim)
        : state_t{gc, old.prior_published_state, d},
          _conflicts{prune(old._conflicts)},
          _modified_vcs{modified_vcs, prune(old._modified_vcs)},
          _blockers{prune(old._blockers)}
      {}
      unpublished_state(gc_token &gc,
                        const unpublished_state &old,
                        gc_ptr<blocking_mod> b,
                        discriminator_type d = discrim)
        : state_t{gc, old.prior_published_state, d},
          _conflicts{prune(old._conflicts)},
          _modified_vcs{prune(old._modified_vcs)},
          _blockers{b, prune(old._blockers)}
      {}

      unpublished_state(gc_token &gc,
                        const unpublished_state &old,
                        drop_conflicts_ctor,
                        discriminator_type d = discrim)
        : state_t{gc, old.prior_published_state, d},
          _modified_vcs{prune(old._modified_vcs)},
          _blockers{prune(old._blockers)}
      {}

      unpublished_state(gc_token &gc,
                        const unpublished_state &old,
                        roll_forward_ctor,
                        timestamp_t as_of,
                        discriminator_type d = discrim)
        : state_t{gc, make_gc<published_state>(as_of, old.prior_published_state), d},
          _conflicts{prune(old._conflicts)},
          _modified_vcs{prune(old._modified_vcs)},
          _blockers{prune(old._blockers)}
      {}

      static auto &descriptor() {
        static gc_descriptor d =
          GC_DESC(unpublished_state)
          .WITH_SUPER(state_t)
          .WITH_FIELD(&unpublished_state::_modified_vcs)
          .WITH_FIELD(&unpublished_state::_conflicts)
          .WITH_FIELD(&unpublished_state::_blockers)
          ;
        return d;
      }

      struct virtuals : state_t::virtuals {
        using impl = unpublished_state;
        gc_ptr<const published_state> most_recent_published(const state_t *self) const override {
          return self->call_non_virtual(&impl::most_recent_published_impl);
        }
        conflict_list conflicts(const state_t *self) const override {
          return self->call_non_virtual(&impl::conflicts_impl);
        }
        gc_ptr<const state_t>
        add_conflict(const state_t *self,
                     const gc_ptr<const conflict> &c) const override
        {
          return self->call_non_virtual(&impl::add_conflict_impl, c);
        }
        gc_ptr<const state_t>
        add_modified(const state_t *self,
                     const gc_ptr<modified_value_chain> &m) const override
        {
          return self->call_non_virtual(&impl::add_modified_impl, m);
        }
        gc_ptr<const state_t>
        add_blocker(const state_t *self,
                    const gc_ptr<blocking_mod> &b) const override
        {
          return self->call_non_virtual(&impl::add_blocker_impl, b);
        }
        gc_ptr<const state_t>
        note_resolved(const state_t *self, const conflict_list &conflicts) const override
        {
          return self->call_non_virtual(&impl::note_resolved_impl, conflicts);
        }
        publish_return publish(const state_t *self,
                               const gc_ptr<const published_state> &new_state,
                               const gc_ptr<const unpublished_state> &last,
                               iso_context &c) const override
        {
          return self->call_non_virtual(&impl::publish_impl, new_state, last, c);
        }

        gc_ptr<state_t> roll_forward(const state_t *self,
                                     timestamp_t as_of) const override
        {
          return self->call_non_virtual(&impl::roll_forward_impl, as_of);
        }
                                       
        
      }; 

      gc_ptr<const published_state> most_recent_published_impl() const {
        return prior_published_state;
      }
      conflict_list conflicts_impl() const {
        return _conflicts;
      }
      gc_ptr<const state_t>
      add_conflict_impl(const gc_ptr<const conflict> &c) const 
      {
        return make_gc<unpublished_state>(*this, c);
      }
      gc_ptr<const state_t>
      add_modified_impl(const gc_ptr<modified_value_chain> &m) const 
      {
        return make_gc<unpublished_state>(*this, m);
      }
      gc_ptr<const state_t>
      add_blocker_impl(const gc_ptr<blocking_mod> &b) const 
      {
        return make_gc<unpublished_state>(*this, b);
      }
      gc_ptr<const state_t>
      note_resolved_impl(const conflict_list &conflicts) const {
        if (_conflicts != conflicts) {
          return GC_THIS;
        }
        return make_gc<unpublished_state>(*this, drop_conflicts_ctor{});
      }

      static void check_for_conflicts(iso_context &ctxt,
                                      gc_ptr<const state_t> &last_checked)
      {
        gc_ptr<const state_t> copy = last_checked;
        last_checked = ctxt.current_state();
        if (last_checked != copy) {
          conflict_list conflicts = last_checked->conflicts();
          if (!conflicts.empty()) {
            throw conflicts;
          }
        }
      }
      
      publish_return publish_impl(const gc_ptr<const published_state> &new_state,
                                  const gc_ptr<const unpublished_state> &last,
                                  iso_context &ctxt) const
      {
        if (!_conflicts.empty()) {
          throw _conflicts;
        }
        gc_ptr<const state_t> last_checked = GC_THIS;

        blocking_mod_list blockers_processed;
        modified_vc_list mvcs_processed;

        if (last != nullptr) {
          blockers_processed = last->_blockers;
          mvcs_processed = last->_modified_vcs;
        }

        _blockers.for_each(blockers_processed,
                           [&](const gc_ptr<blocking_mod> &bm) {
                             if (bm->pending()) {
                               check_for_conflicts(ctxt, last_checked);
                               bm->remove();
                             }
                           });
        _modified_vcs.for_each(mvcs_processed,
                               [&](const gc_ptr<modified_value_chain> &mvc) {
                                 if (!mvc->cleared()) {
                                   mvc->chain->prepare_for_publish(mvc->in_msv,
                                                                   new_state);
                                 }
                               });
        return std::make_pair(GC_THIS, prior_published_state);
      }

      gc_ptr<state_t> roll_forward_impl(timestamp_t as_of) const {
        return make_gc<unpublished_state>(*this, roll_forward_ctor{}, as_of);
      }

    }; // unpublished_state

    struct iso_context::in_process_inbound_publish : gc_allocated {
      std::atomic<gc_ptr<iso_context>> child;

      bool is_done() const {
        return child.load() == nullptr;
      }

      void mark_done() {
        child = nullptr;
      }

      in_process_inbound_publish(gc_token &gc,
                                 const gc_ptr<iso_context> &c)
        : gc_allocated{gc}, child{c}
      {}

      static const auto &descriptor() {
        static gc_descriptor d =
	  GC_DESC(in_process_inbound_publish)
	  .WITH_FIELD(&in_process_inbound_publish::child);
        return d;
      }

    }; // in_process_inbound_publish

    
    

    gc_ptr<const iso_context::state_t>
    iso_context::published_state::
    add_conflict_impl(const gc_ptr<const conflict> &c) const 
    {
      return make_gc<unpublished_state>(GC_THIS, c);
    }
    gc_ptr<const iso_context::state_t>
    iso_context::published_state::
    add_modified_impl(const gc_ptr<modified_value_chain> &m) const 
    {
      return make_gc<unpublished_state>(GC_THIS, m);
    }
    gc_ptr<const iso_context::state_t>
    iso_context::published_state::
    add_blocker_impl(const gc_ptr<blocking_mod> &b) const 
    {
      return make_gc<unpublished_state>(GC_THIS, b);
    }

    gc_ptr<view>
    iso_context::find_shadow(const gc_ptr<view> &v) {
      gc_ptr<shadow_node> new_shadow = nullptr;
      while (true) {
        gc_ptr<shadow_node> current_shadows = _shadows;
        for (gc_ptr<shadow_node> sn = current_shadows;
             sn != nullptr;
             sn = sn->next)
          {
            if (sn->base == v) {
              return sn->shadow;
            }
          }
        if (new_shadow == nullptr) {
          const gc_ptr<view> psv = is_global() ? v : _parent->shadow(v);
          const gc_ptr<view> spsv = (psv == v)
            ? make_gc<view>(GC_THIS, psv) : shadow(psv);
          new_shadow = make_gc<shadow_node>(v, spsv, current_shadows);
          // std::cout << GC_THIS << "'s"
          //           << " shadow for " << v
          //           << " is " << spsv
          //           << std::endl;
        } else {
          new_shadow->next = current_shadows;
        }
        if (ruts::try_change_value(_shadows, current_shadows, new_shadow)) {
          return new_shadow->shadow;
        }
      }
    }


    void
    iso_context::block_inbound_publication(const gc_ptr<blocking_mod> &bm) {
      /*
       * Back when we were serializing on all modifications to an MSV,
       * we could make the assumption that if there were no
       * publishable children when we started, we didn't have to do
       * anything here, since if a child was created while the
       * modification was in progress and there was an attempt to
       * publish it, it would either have no values of its own or it
       * would have finished the blocking modification before
       * asserting its value.  Now that we're only serializing on
       * specific value chains, we can no longer make that assumption.
       */
      _block_inbound.prune_and_push(bm, [](const auto &b) {
          return !b->pending();
        });
      _in_process.prune_initial_satisfying([](const auto &ip) {
          return ip->is_done();
        });
      _in_process.for_each([&](const auto &ip) {
          gc_ptr<iso_context> child = ip->child;
          if (child != nullptr) {
            child->block_publication(bm);
          }
        });
    }

    gc_ptr<iso_context::in_process_inbound_publish>
    iso_context::prepare_for_publish(const gc_ptr<iso_context> &child)
    {
      /*
       * We want to hold off on installing the ipip as long as
       * possible, because as soon as we do, we're going to start
       * forwarding block_inbound_publication() requests, which will
       * require state creations.  So we first drain the stack until
       * it's at least momentarily empty.
       */
      drain_inbound_blockers(child);
      gc_ptr<in_process_inbound_publish>
        ip = make_gc<in_process_inbound_publish>(child);
      _in_process.prune_and_push(ip, [](const auto &ip) {
            return ip->is_done();
          });
      /*
       * But there may have been some new ones that arrived in between
       * the time we finished draining and the time we put the ipip on
       * the stack.  (Hopefully not many.)  So we'll drain again.  Any
       * that come after we push the ipip will also be looked at by
       * the child.
       */
      drain_inbound_blockers(child);
      return ip;
    }

    void
    iso_context::drain_inbound_blockers(const gc_ptr<iso_context> &child) {
      gc_ptr<const state_t> last_state = nullptr;
      _block_inbound.apply_and_pop_all([&](const gc_ptr<blocking_mod> &bm) {
          if (bm->pending()) {
            /*
             * If the child has received conflicts, we want to short circuit.
             */
            gc_ptr<const state_t> s = child->current_state();
            if (s != last_state) {
              last_state = s;
              conflict_list conflicts = s->conflicts();
              if (!conflicts.empty()) {
                throw conflicts;
              }
            }
            bm->remove();
          }
        });
    }

    gc_ptr<publication_attempt>
    iso_context::publish()
    {
      if (!is_publishable()) {
        throw unpublishable_context_ex{};
      }
      struct already_published{};
      gc_ptr<published_state> new_state = make_gc<published_state>();
      gc_ptr<const unpublished_state> last;
      timestamp_t start_time = *current_version;
      timestamp_t publish_time;
      try {
        /*
         * We want to hold off calling prepare() as long as possible,
         * because as soon as we do, we'll get blockers passed down.
         * So we wait until we get past a state publish call and the
         * state hasn't changed.  (Yes, this means peeking ahead, but
         * it's probably worth it.)
         */
        gc_ptr<in_process_inbound_publish> ip = nullptr;
        auto invalidate = ruts::cleanup([&]{
            if (ip != nullptr) {
              ip->mark_done();
            }
          });
        update_state([&, this](const auto &current) {
            gc_ptr<const published_state> new_prior;
            std::tie(last, new_prior) = current->publish(new_state, last, *this);
            if (new_prior->timestamp() > start_time) {
              /*
               * If we see a new prior with a timestamp after we
               * started, it means that there was a successful
               * publication after we started.  We'll claim that as
               * our own.
               */
              publish_time = new_prior->timestamp();
              throw already_published{};
            }
            publish_time = ++(*current_version);
            new_state->set_ts_and_prior(publish_time, new_prior);
            /*
             * If it's already changed, we don't bother creating the
             * ipip, since the most likely scenario is that it got a
             * conflict.
             */
            if (ip == nullptr && _state.load() == current) {
              ip = this->parent()->prepare_for_publish(GC_THIS);
            }
            return new_state;
          });
        new_state->mark_published();
      } catch (already_published) {
        /*
         * Nothing to do.
         */
      } catch (const conflict_list &conflicts) {
        return make_gc<publication_attempt>(GC_THIS, conflicts, start_time);
      }
      return make_gc<publication_attempt>(GC_THIS, conflict_list{}, publish_time);
    }

    struct publication_attempt::redo_graph : public gc_allocated {
      struct node : public gc_allocated {
        const gc_ptr<task> _task;
        gc_ptr<node> _fwd = nullptr;
        gc_vector<gc_ptr<node>> _follows;

        node(gc_token &gc, const gc_ptr<task> &t)
          : gc_allocated{gc}, _task{t}
        {}

        static const auto &descriptor() {
          using this_class = node;
          static gc_descriptor d =
            GC_DESC(this_class)
            .WITH_FIELD(&node::_task)
            .WITH_FIELD(&node::_fwd)
            .WITH_FIELD(&node::_follows)
            ;
          return d;
        }

        gc_ptr<node> deref() {
          gc_ptr<node> n = GC_THIS;
          while (n->_fwd != nullptr) {
            n = n->_fwd;
          }
          return n;
        }

        /*
         * Ensure that there is only a single reference to each node
         * and that only unsubsumed nodes are pointed to.
         */
        void prune_follows() {
          std::unordered_set<gc_ptr<node>> pruned;
          for (const gc_ptr<node> &f : _follows) {
            pruned.insert(f->deref());
          }
          _follows.assign(pruned.begin(), pruned.end());
        }


      }; // node
      
      friend class publication_attempt;
      bool _prepared = false;
      gc_vector<node> _nodes;

      using vc_mvc_pair = std::pair<gc_ptr<value_chain>, gc_ptr<modified_value_chain>>;
      gc_vector<vc_mvc_pair> _vcs;

      struct graph {
        /*
         * The task map both keeps a mapping from task to their current
         * proxy nodes and also holds onto all tasks and nodes (by using
         * external_gc_ptr).
         */
        std::unordered_map<external_gc_ptr<task>, external_gc_ptr<node> >
          task_to_node;
        /*
         * Given the task map, which holds onto everything looked up
         * as an external_gc_ptr, we can use normal gc_ptrs in std
         * data structures for the rest of this.
         */
        /*
         * Tasks to be processed.
         */
        std::queue<gc_ptr<task>> agenda;
        /*
         * Tasks that have been added to the agenda.  We only add each
         * one once.
         */
        std::unordered_set<gc_ptr<task>> added_to_agenda;
        /*
         * Nodes to redo
         */
        std::unordered_set<gc_ptr<node>> to_redo;

        /*
         * VCs that will need to be visited and their associated MSVs.
         * VCs that come from conflicts have no associated MSVs.
         */
        std::unordered_map<gc_ptr<value_chain>,
                           gc_ptr<modified_value_chain>> vcs_to_visit;
        

        gc_ptr<node> lookup(const gc_ptr<task> &t) {
          auto n = task_to_node[t];
          gc_ptr<node> dn = n == nullptr ? make_gc<node>(t) : n->deref();
          //          std::cout << "Node is " << dn << std::endl;
          /*
           * Update if it's changed
           */
          if (n != dn) {
            task_to_node[t] = dn;
          }
          return dn;
        }

        void add_to_agenda(const gc_ptr<task> &t) {
          //          std::cout << "Adding task to agenda: " << t << std::endl;
          /*
           * First we call lookup() to ensure that the task will stick
           * around.
           */
          lookup(t);
          /*
           * We don't bother enqueing tasks that have already been
           * processed.
           */
          if (added_to_agenda.insert(t).second) {
            //            std::cout << "Pushing to agenda" << std::endl;
            agenda.push(t);
          }
        }

        graph(const gc_ptr<iso_context> &ctxt,
              const conflict_list &conflicts)
        {
          /*
           * First we add all of the redo tasks for the conflicts.
           * The VCs for these conflicts are necessarily in the tasks'
           * list of MVCs.  If there are any conflicts marked as
           * having been resolved, we skip them.
           */
          conflicts.for_each([this](const gc_ptr<const conflict> &c) {
              if (!c->is_resolved()) {
                this->add_to_agenda(c->redo_task());
              }
            });
          if (!agenda.empty()) {
            /*
             * If this resulted in any tasks being added, we add all
             * of the tasks marked to be unconditionally redone.
             */
            ctxt->for_each_unconditional_redo([this](const auto &t) {
                this->add_to_agenda(t);
              });
          }
          /*
           * Now we process the agenda until it's empty.
           * add_to_agenda() ensures that no task gets added more than
           * once.
           */
          while (!agenda.empty()) {
            gc_ptr<task> t = agenda.front();
            agenda.pop();
            //            std::cout << "Need to redo task " << t << std::endl;
            /*
             * Mark this task as needing to be redone.  It may be part
             * of another task to be redone, but this means that we
             * know that any writes it makes should be skipped over
             * when rolling forward.
             */
            t->set_needs_redo(true);

            gc_ptr<node> n = lookup(t);
            /*
             * If this task hasn't been subsumed, add it to the set of
             * tasks to redo.
             */
            if (n->_task == t) {
              //              std::cout << "Adding to todo list: node " << n << std::endl;
              to_redo.insert(n);
            }
            /*
             * Dependent tasks get added to the agenda, and their
             * nodes are added to this node's follow set.
             */
            t->for_each_dependent_task([&,this](const gc_ptr<task> &dt) {
                add_to_agenda(dt);
                gc_ptr<node> dn = lookup(dt);
                n->_follows.push_back(dn);
              });
            /*
             * All of the VCs for the task are added to the set, along
             * with their associated MSVs.  This may clobber a null
             * from a conflict.
             */
            t->for_each_modification([&](const gc_ptr<modified_value_chain> &mvc) {
                // std::cout << "Need to visit VC " << mvc->chain << std::endl;
                if (!mvc->cleared()) {
                  vcs_to_visit[mvc->chain] = mvc;
                }
              });
            
            /*
             * Subtasks get added to the agenda.  Any follows their
             * nodes have get added to this node.  Then their node
             * forwards to us.  Finally, their node is taken out of
             * the to_redo set (if it was added).
             *
             * When they come up in the agenda, they'll find this node
             * (or something higher), and their subtasks will also get
             * added to it.
             */
            t->for_each_subtask([&,this](const gc_ptr<task> &st) {
                add_to_agenda(st);
                gc_ptr<node> sn = lookup(st);
                for (const gc_ptr<node> &f : sn->_follows) {
                  n->_follows.push_back(f);
                }
                sn->_fwd = n;
                //                std::cout << "Removing subsumed node " << sn << std::endl;
                to_redo.erase(sn);
              });
          }
          /*
           * Finally, we need to prune the follow sets of the nodes in
           * the to_redo set.  They may contain multiple copies of the
           * same node or nodes that forward to one another.
           */
          for (const gc_ptr<node> &n : to_redo) {
            n->prune_follows();
          }
          //          std::cout << "Graph constructed" << std::endl;
        }

      }; // graph


      
      redo_graph(gc_token &gc,
                 const gc_ptr<iso_context> &ctxt,
                 const conflict_list &conflicts)
        : gc_allocated{gc}
      {
        //        std::cout << "Building redo graph" << std::endl;
        graph g(ctxt, conflicts);
        _nodes.assign(g.to_redo.begin(),
                      g.to_redo.end());
        _vcs.reserve(g.vcs_to_visit.size());
        for (const auto &e : g.vcs_to_visit) {
          _vcs.emplace_back(e.first, e.second);
        }
        //        std::cout << "Redo tasks:" << std::endl;
        // std::for_each(_nodes.begin(), _nodes.end(), [](const auto &n) {
        //     std::cout << "Task " << n->_task << std::endl;
        //   });
      }

      static const auto &descriptor() {
        using this_class = redo_graph;
        static gc_descriptor d =
	  GC_DESC(this_class)
          .WITH_FIELD(&this_class::_prepared)
          .WITH_FIELD(&this_class::_nodes)
          .WITH_FIELD(&this_class::_vcs)
          ;
        return d;
      }


      void prepare_for_redo() {
        if (!_prepared) {
          /*
           * We walk through the VCs and ask them to prepare.
           */
          for (const auto &p : _vcs) {
            gc_ptr<value_chain> vc = p.first;
            gc_ptr<modified_value_chain> mvc = p.second;
            if (!mvc->cleared()) {
              vc->prepare_for_redo(mvc);
            }
          }
          /*
           * Then we clear info from the tasks to be redone (since we've
           * just undone everything they did).
           */
          for (const auto &n : _nodes) {
            n->_task->prepared_for_redo();
          }
          /*
           * Finally, we mark ourselves as prepared so we're ready to
           * redo the tasks.
           */
          _prepared = true;
        }
      }
      
    }; // redo_graph

    gc_ptr<publication_attempt::redo_graph>
    publication_attempt::get_redo_graph() const {
      auto rr = ruts::try_cas(_redo_graph,
                              [](const auto &old) {
                                return old == nullptr;
                              },
                              [this](const auto &) {
                                return make_gc<redo_graph>(_context, _conflicts);
                              });
      return rr.resulting_value();
    }

    std::size_t
    publication_attempt::n_to_redo() const {
      if (succeeded()) {
        return 0;
      } else {
        return get_redo_graph()->_nodes.size();
      }
    }

    std::vector<gc_ptr<task>>
    publication_attempt::get_redo_task_list() const {
      using namespace std;
      
      const auto &nodes = get_redo_graph()->_nodes;
      // std::cout << "Building redo task list" << std::endl;
      vector<gc_ptr<task>> v;
      v.reserve(nodes.size());
      // std::cout << "Reserved space in vector" << std::endl;
      transform(nodes.begin(), nodes.end(), back_inserter(v),
                [](const auto &n) {
                  return n->_task;
                });
      // std::cout << "Filled vector" << std::endl;
      return v;
    }

    std::vector<gc_ptr<task>>
    publication_attempt::redo_tasks_by_start_time() const {
      using namespace std;
      
      vector<gc_ptr<task>> v = get_redo_task_list();
      sort(v.begin(), v.end(),
           [](const auto &t1, const auto &t2) {
             return t1->start_tick() < t2->start_tick();
           });
      // for_each(v.begin(), v.end(), [](const auto &t) {
      //     std::cout << "Will redo task " << t << std::endl;
      //   });
      return v;
    }

    bool
    publication_attempt::prepare_for_redo() const {
      auto g = get_redo_graph();
      if (g->_nodes.size() == 1 && !g->_nodes[0]->_task->is_redoable()) {
        /*
         * If there's only one node and it's not redoable, it must be
         * the top-level task, because any other non-redoable task
         * would've just added its own parent to the list.
         */
        assert(g->_nodes[0]->_task->is_top_level());
        return false;
      }
      g->prepare_for_redo();
      _conflicts.for_each([](auto &c) {
          c->mark_resolved();
        });
      if (_context->is_snapshot()) {
        /*
         * If this is a snapshot, we the context forward to now.
         *
         * TODO: What about things that happened between the time we
         * rolled individual VCs and now?
         */
        timestamp_t now = ++(*current_version);
        _context->roll_snapshot_forward(now);
      }
      _context->note_resolved(_conflicts);
      return true;
    };

    void
    publication_attempt::redo(const gc_ptr<task> &) const {
      /*
       * I'm really not sure what to do here.  Probably just push the
       * task.
       */
      // TODO
    };

    void
    iso_context::state_t::init_vf_table(vf_table &t)
    {
      t.bind<iso_context::published_state>(); 
      t.bind<iso_context::unpublished_state>(); 
    }
  }
}


