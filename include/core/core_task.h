/*
 *
 *  Managed Data Structures
 *  Copyright Â© 2016 Hewlett Packard Enterprise Development Company LP.
 *
 *  This program is free software: you can redistribute it and/or modify
 *  it under the terms of the GNU Lesser General Public License as published by
 *  the Free Software Foundation, either version 3 of the License, or
 *  (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU Lesser General Public License for more details.
 *
 *  You should have received a copy of the GNU Lesser General Public License
 *  along with this program.  If not, see <http://www.gnu.org/licenses/>.
 *
 *  As an exception, the copyright holders of this Library grant you permission
 *  to (i) compile an Application with the Library, and (ii) distribute the 
 *  Application containing code generated by the Library and added to the 
 *  Application during this compilation process under terms of your choice, 
 *  provided you also meet the terms and conditions of the Application license.
 *
 */

/*
 * core_msv.h
 *
 *  Created on: Oct 21, 2014
 *      Author: evank
 */

#ifndef CORE_TASK_H_
#define CORE_TASK_H_

#include "core/core_fwd.h"
#include "core/core_kind.h"
#include "core/core_globals.h"
#include "mpgc/gc_stack.h"
#include "ruts/packed_word.h"

namespace mds {
  namespace core {

    class no_prior_task_ex {};
    class thread_base_task_unset_ex {};

    class task : public exportable, public with_uniform_id {

      struct alignas(16) bounds {
        std::uint64_t start = 0;
        std::int64_t end_or_minus_depth = 0;

        bounds() noexcept {}

        static const auto &descriptor() {
          using this_class = bounds;
          static gc_descriptor d =
            GC_DESC(this_class)
            .WITH_FIELD(&this_class::start)
            .WITH_FIELD(&this_class::end_or_minus_depth)
            ;
          return d;
        }

        bool running() const {
          return end_or_minus_depth < 0;
        }

        bounds &push() {
          if (running()) {
            end_or_minus_depth--;
          } else {
            end_or_minus_depth = -1;
            start = (*next_task_number)++;
          }
          return *this;
        }

        bounds &pop() {
          assert(running());
          if (++end_or_minus_depth == 0) {
            end_or_minus_depth = *next_task_number;
          }
          return *this;
        }

        bounds &reset() {
          start = 0;
          end_or_minus_depth = 0;
          return *this;
        }

        uint64_t start_time() const {
          return start;
        }

        uint64_t end_time() const {
          assert(!running());
          return end_or_minus_depth;
        }

        uint64_t depth() const {
          return running() ? -end_or_minus_depth : 0;
        }
      };


      /*
       * We assume that only one thread at a time will try to initiate
       * a redo for a given context.  So we don't bother making
       * _needs_redo atomic.
       *
       * Note that because of the alignment of _bounds (needed to
       * ensure that it can be put in an atomic), we need to have a
       * single word before our fields start in order to ensure that
       * the coverage check for the descriptor will see all words
       * touched.  So we stick the booleans here.
       */
      bool _needs_redo = false;
      bool _redoable = true;
      const bool _publishable;
      /*
       * TODO.  Yes, there's a problem if a thread pushes into its
       * bounds and then dies before popping.  We'll just have to
       * allow that to happen.
       */
      alignas(16) std::atomic<bounds> _bounds;
      const gc_ptr<iso_context> _context;
      const gc_ptr<task> _parent;
      gc_atomic_stack<gc_ptr<task>> _subtasks;
      gc_atomic_stack<gc_ptr<task>>  _dependent_tasks;
      gc_atomic_stack<gc_ptr<modified_value_chain>> _modifications;
      const std::size_t _pad = 0;

      class prevailing_stack {
        /*
         * It's safe to hold _current as a gc_ptr, since we guarantee
         * that it's also in the stack as an external_gc_ptr.
         */
        gc_ptr<task> _current;
        std::stack<external_gc_ptr<task>> _stack;
        static external_gc_ptr<task> &base_task() {
          static thread_local external_gc_ptr<task> t;
          return t;
        }
      public:
        prevailing_stack()
          : _current{base_task()}
        {
          if (_current == nullptr) {
            throw thread_base_task_unset_ex{};
          }
          /*
           * Once it's read, we don't have to hold onto it.
           */
          base_task() = nullptr;
          _stack.emplace(_current);
        }

        template <typename Fn>
        static void ensure_base_task_set(Fn &&fn) {
          auto &t = base_task();
          if (t == nullptr) {
            t = std::forward<Fn>(fn)();
          }
        }
        

        operator gc_ptr<task>() const {
          return _current;
        }

        gc_ptr<task> push(const gc_ptr<task> &t) {
          _current = t;
          _stack.emplace(t);
          return t;
        }

        gc_ptr<task> pop() {
          if (_stack.empty()) {
            throw no_prior_task_ex{};
          } 
          _stack.pop();
          _current = _stack.top();
          return _current;
        }
        
      };

      static prevailing_stack &_prevailing() {
        static thread_local prevailing_stack ps;
        return ps;
      }

      gc_ptr<task> child_task() {
        if (_publishable) {
          gc_ptr<task> t = make_gc<task>(private_ctor{}, _context, GC_THIS);
          // std::cout << "Publishable, created new task " << t << std::endl;
          return t;
        } else {
          // std::cout << "Unpublishable, returning self: " << GC_THIS << std::endl;
          return GC_THIS;
        }
      }

      struct private_ctor {};
    public:
      task(gc_token &gc,
           private_ctor,
           const gc_ptr<iso_context> &c,
           const gc_ptr<task> &p);

      static gc_ptr<task> for_context(const gc_ptr<iso_context> &c) {
        return make_gc<task>(private_ctor{}, c, nullptr);
      }


      static const auto &descriptor() {
        static gc_descriptor d =
	  GC_DESC(task)
          .WITH_SUPER(exportable)
	  .WITH_SUPER(with_uniform_id)
          .WITH_FIELD(&task::_bounds)
          .WITH_FIELD(&task::_context)
          .WITH_FIELD(&task::_parent)
          .WITH_FIELD(&task::_subtasks)
          .WITH_FIELD(&task::_publishable)
          .WITH_FIELD(&task::_dependent_tasks)
          .WITH_FIELD(&task::_modifications)
          .WITH_FIELD(&task::_needs_redo)
          .WITH_FIELD(&task::_redoable)
          .WITH_FIELD(&task::_pad);
          ;
        return d;
      }


      std::size_t start_tick() const {
        return _bounds.load().start_time();
      }

      std::size_t end_tick() const {
        return _bounds.load().end_time();
      }

      /*
       * For a task in a publishable context, this will be the number
       * of threads it's running in.  For a task in an unpublishable
       * context, it will also include the number of pseudo-child
       * pushes in each thread.
       */
      std::size_t num_outstanding_pushes() const {
        return _bounds.load().depth();
      }

      void add_dependent_task(const gc_ptr<task> &t) {
        _dependent_tasks.push_if_empty_or(t, [&](const auto &first) {
            return first != t;
          });
      }

      template <typename Fn>
      void for_each_dependent_task(Fn&& fn) const {
        _dependent_tasks.for_each(std::forward<Fn>(fn));
      }

      template <typename Fn>
      void for_each_subtask(Fn&& fn) const {
        _subtasks.for_each(std::forward<Fn>(fn));
      }

      template <typename Fn>
      void for_each_modification(Fn&& fn) const {
        _modifications.for_each(std::forward<Fn>(fn));
      }

      void unconditionally_redo();
      
      void add_modification(const gc_ptr<modified_value_chain> &mvc) {
        /*
         * If we're not publishable, there's no need to pay attention
         * to modifications.
         */
        if (_publishable) {
          _modifications.push(mvc);
        }
      }

      gc_ptr<iso_context> get_context() const {
        return _context;
      }

      gc_ptr<task> get_parent() const {
        return _parent;
      }

      bool is_top_level() const {
        return _parent == nullptr;
      }

      bool is_redoable() const {
        return _redoable;
      }

      void set_not_redoable() {
        /*
         * The way we make a task not redoable is to make its parent
         * dependent on it, so that if it needs to be redone, its
         * parent does, also.  If we're already at top level, we just
         * set a flag so that we don't bother trying.
         */
        _redoable = false;
        if (_parent != nullptr) {
          add_dependent_task(_parent);
        }
      }

      template <typename Fn>
      static void init_thread_base_task(Fn &&fn) {
        prevailing_stack::ensure_base_task_set(std::forward<Fn>(fn));
      }

      /*
       * Throws thread_base_task_unset_ex if init_thread_base_task()
       * hasn't been called in this thread.  (Or if its function
       * returned null.)
       */
      static gc_ptr<task> prevailing() {
        return _prevailing();
      }

      gc_ptr<task> push() {
        ruts::cas_loop(_bounds, [](auto b) {
            return b.push();
          });
        auto t = _prevailing().push(GC_THIS);
        // std::cout << "Pushing to existing " << t
        //           << " (asked for " << GC_THIS << ")"
        //           << std::endl;
        return t;
      }

      static gc_ptr<task> push_new() {
        return prevailing()->child_task()->push();
      }

      static gc_ptr<task> pop() {
        gc_ptr<task> t = _prevailing();
        ruts::cas_loop(t->_bounds, [&](auto b) {
            return b.pop();
          });
        return _prevailing().pop();
      }

      template <typename Fn, typename...Args>
      auto execute(Fn&& fn, Args&&... args) {
        auto pop_after = ruts::cleanup(pop);
        push();
        return std::forward<Fn>(fn)(std::forward<Args>(args)...);
      }
      
      template <typename Fn, typename...Args>
      auto execute_in_new(Fn&& fn, Args&&... args) {
        return prevailing()->execute(std::forward<Fn>(fn),
                                     std::forward<Args>(args)...);
      }

      void set_needs_redo(bool b) {
        _needs_redo = b;
      }

      bool needs_redo() const {
        return _needs_redo;
      }

      /*
       * prepared_for_redo() is called once the publication_attempt
       * has finished prepare_for_redo() and everything has been
       * rolled forward.  It's assumed that the task will now be
       * redone, so we don't have to worry about any of the remnants
       * of the past execution.  We don't (currently) go down into the
       * subtasks, as the expectation is that they will be dropped on
       * the floor.
       */
      void prepared_for_redo() {
        if (_needs_redo) {
          _subtasks.clear();
          _dependent_tasks.clear();
          _modifications.clear();
          _needs_redo = false;
        }
      }


    }; // task
  }
}



#endif /* CORE_TASK_H_ */
