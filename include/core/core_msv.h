/*
 *
 *  Managed Data Structures
 *  Copyright Â© 2016 Hewlett Packard Enterprise Development Company LP.
 *
 *  This program is free software: you can redistribute it and/or modify
 *  it under the terms of the GNU Lesser General Public License as published by
 *  the Free Software Foundation, either version 3 of the License, or
 *  (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU Lesser General Public License for more details.
 *
 *  You should have received a copy of the GNU Lesser General Public License
 *  along with this program.  If not, see <http://www.gnu.org/licenses/>.
 *
 *  As an exception, the copyright holders of this Library grant you permission
 *  to (i) compile an Application with the Library, and (ii) distribute the 
 *  Application containing code generated by the Library and added to the 
 *  Application during this compilation process under terms of your choice, 
 *  provided you also meet the terms and conditions of the Application license.
 *
 */

/*
 * core_msv.h
 *
 *  Created on: Oct 21, 2014
 *      Author: evank
 */

#ifndef CORE_MSV_H_
#define CORE_MSV_H_

#include "ruts/packed_word.h"
#include "core/core_task.h"
#include "core/core_context.h"

namespace mds {
  namespace core {
    enum class modify_op {
      set, clear, add, sub, mul, div,
        current_val, 
        frozen_current, roll_forward
        }; // modify_op

    enum class ret_mode {
      resulting_val, prior_val
        }; // ret_mode
    
    struct incompatible_modify_op {
      modify_op op;
      incompatible_modify_op(modify_op o) : op{o} {}
    }; // incompatible_modify_op

    struct modification_ex {};
    struct computation_ex : modification_ex {};
    struct div_by_zero_ex : computation_ex {};
    struct guard_failure_ex : modification_ex {};
    struct unknown_modification_ex : modification_ex {
      std::size_t status_code;
      unknown_modification_ex(std::size_t c) : status_code{c} {}
    };
      


    class value : public gc_allocated {
      using data_t = ruts::packed_word<std::uint64_t,
                                       ruts::pw_field<bool, 1>,
                                       ruts::pw_field<bool, 1>,
                                       ruts::pad,
                                       ruts::pw_field<timestamp_t, 62>>;
      constexpr static std::size_t marks_publish_field = 0;
      constexpr static std::size_t single_modifier_field = 0;
      constexpr static std::size_t timestamp_field = 2;
      data_t _data;
    public:
      const gc_ptr<task> write_task;
      value(gc_token &gc, timestamp_t time,
            bool marks_publish, bool single_modifier,
            const gc_ptr<task> &t = nullptr)
        : gc_allocated{gc},
          _data{marks_publish, single_modifier, time},
          write_task{t}
      {
        assert(marks_publish == (t == nullptr));
      }
      const static auto &descriptor() {
        static gc_descriptor d =
          GC_DESC(value)
          .WITH_FIELD(&value::_data)
          .WITH_FIELD(&value::write_task);
        return d;
      }
      bool marks_publish() const {
        return _data.field<marks_publish_field>();
      }

      bool single_modifier() const {
        return _data.field<single_modifier_field>();
      }
      
      timestamp_t timestamp() const {
        return _data.field<timestamp_field>();
      }
    }; // value

    class redo_task_set : public gc_allocated {
      std::atomic<gc_ptr<conflict>> _conflict;
      const gc_ptr<task> _redo_task;
      const gc_ptr<task> _source_task;
      /*
       * May need the ability to have extended lists of these tasks.
       */
    public:
      redo_task_set(gc_token &gc,
                    const gc_ptr<task> &rt,
                    const gc_ptr<task> &st)
        : gc_allocated{gc},
          _conflict{nullptr},
          _redo_task{rt}, _source_task{st}
      {}

      const static auto &descriptor() {
        static gc_descriptor d =
          GC_DESC(redo_task_set)
          .WITH_FIELD(&redo_task_set::_conflict)
          .WITH_FIELD(&redo_task_set::_redo_task)
          .WITH_FIELD(&redo_task_set::_source_task);
        return d;
      }

      void set_conflict(const gc_ptr<conflict> c,
                        const gc_ptr<iso_context> &ctxt)
      {
        /*
         * We have to be a bit careful here.  We've got the context
         * blocked at this point, so if we die, the context will
         * finish us up before it can publish.  But if we made adding
         * the conflict to the context contingent on setting it here,
         * we could successfully set here and then die before adding
         * to the context, and it would think it was done and wouldn't
         * do it.  And if we don't check null, then both would try to
         * add to the context.  So we do it in a try_cas, which allows
         * us to do both cheaply.  We can still get multiples, but it
         * should be less common.
         */
        ruts::try_cas(_conflict,
                      [](const auto &old) {
                        return old == nullptr;
                      },
                      [&](const auto &old) {
                        ctxt->add_conflict(c);
                        return c;
                      });
        /*
         * If that fails, it means somebody already added one.
         */
      }

      gc_ptr<task> source_task() const {
        return _source_task;
      }

      gc_ptr<task> redo_task() const {
        return _redo_task;
      }

      gc_ptr<conflict> get_conflict() const {
        return _conflict;
      }

    }; // redo_task_set

    /*
     * The discriminator is of the form Kind*10+ViewType*2+ModType.
     */
    class value_chain : public gc_allocated_with_virtuals<value_chain> {
      using base = gc_allocated_with_virtuals<value_chain>;

    protected:
      static constexpr discriminator_type to_disc(mod_type mt, view_type vt, kind k) {
        return ruts::index(k)*10+ruts::index(vt)*2+ruts::index(mt);
      }

      using published_state = iso_context::published_state;
      const gc_ptr<view> _view;

    public:
      static void init_vf_table(typename base::vf_table &);
      
      value_chain(gc_token &gc, const gc_ptr<view> &v, discriminator_type d)
        : base{gc, d}, _view{v}
      {}
      const static auto &descriptor() {
        static gc_descriptor d =
          GC_DESC(value_chain)
          .WITH_SUPER(base)
          .WITH_FIELD(&value_chain::_view)
          ;
        return d;
      }

      struct virtuals : virtuals_base {
        using impl = value_chain;
        virtual void receive_conflict(value_chain *self, const gc_ptr<blocking_mod> &bm) = 0;
        virtual void do_rollup(value_chain *self, timestamp_t closed_ts) = 0;
        virtual void prepare_for_publish(value_chain *self,
                                         const gc_ptr<msv> &in_msv,
                                         const gc_ptr<const published_state> &new_state) = 0;
        virtual void add_contingent_child(value_chain *self, const gc_ptr<value_chain> &child) = 0;
        virtual gc_ptr<value_chain> get_parent(const value_chain *self) const = 0;
        virtual void prepare_for_redo(value_chain *self,
                                      const gc_ptr<modified_value_chain> &mvc)
        {
          return self->call_non_virtual(&impl::prepare_for_redo_impl, mvc);
        }

        /*
         * This is specifically the publish state after the timestamp
         * associated with a child being published.  It can only be
         * non-null for a publishable VC and should probably fail an
         * assertion for a read-only VC.  Note that we only care about
         * it if the chain is closed.  If it's open, its prior publish
         * has already been taken care of.  This is to check for
         * publish in a closed parent after a child.
         */
        virtual gc_ptr<const iso_context::published_state> pub_state_after(const value_chain *self,
                                                                           timestamp_t ts) const
        {
          return self->call_non_virtual(&impl::pub_state_after_impl, ts);
        }

        virtual void note_publishable_child(value_chain *self) {
          // By default, nothing to do.
        }
      };

      void receive_conflict(const gc_ptr<blocking_mod> &bm = nullptr) {
        call_virtual(this, &virtuals::receive_conflict, bm);
      }

      void do_rollup(timestamp_t closed_ts) {
        call_virtual(this, &virtuals::do_rollup, closed_ts);
      }

      void prepare_for_publish(const gc_ptr<msv> &in_msv,
                               const gc_ptr<const published_state> &new_state)
      {
        call_virtual(this, &virtuals::prepare_for_publish, in_msv, new_state);
      }

      void add_contingent_child(const gc_ptr<value_chain> &child) {
        call_virtual(this, &virtuals::add_contingent_child, child);
      }

      gc_ptr<value_chain> get_parent() const {
        return call_virtual(this, &virtuals::get_parent);
      }

      void prepare_for_redo(const gc_ptr<modified_value_chain> &mvc)
      {
        return call_virtual(this, &virtuals::prepare_for_redo, mvc);
      }

      gc_ptr<const iso_context::published_state> pub_state_after(timestamp_t ts) const {
        return call_virtual(this, &virtuals::pub_state_after, ts);
      }

      void note_publishable_child() {
        call_virtual(this, &virtuals::note_publishable_child);
      }

      gc_ptr<const iso_context::published_state> pub_state_after_impl(timestamp_t ts) const
      {
        // Default behavior
        return nullptr;
      }
      

      void prepare_for_redo_impl(const gc_ptr<modified_value_chain> &mvc)
      {
        assert(ruts::fail("prepare_for_redo() called on unpublishable VC"));
      }

      gc_ptr<view> get_view() const {
        return _view;
      }

      gc_ptr<iso_context> get_context() const {
        return get_view()->context;
      }
    }; // value_chain

    struct modified_value_chain : public gc_allocated {
    private:
      bool _cleared = false;
    public:
      weak_gc_ptr<value_chain> chain;
      weak_gc_ptr<msv> in_msv;

      modified_value_chain(gc_token &gc,
                           const gc_ptr<value_chain> &vc,
                           const gc_ptr<msv> &m)
        : gc_allocated{gc}, chain{vc}, in_msv{m}
      {}

      const static auto &descriptor() {
        static gc_descriptor d =
          GC_DESC(modified_value_chain)
          .WITH_FIELD(&modified_value_chain::chain)
          .WITH_FIELD(&modified_value_chain::in_msv)
          .WITH_FIELD(&modified_value_chain::_cleared)
          ;
        return d;
      }

      bool cleared() const {
        return _cleared;
      }

      void clear() {
        in_msv = nullptr;
        chain = nullptr;
        _cleared = true;
      }
    }; // modified_value_chain

    class pending_rollup : public gc_allocated {
      /*
       * We need a small object before the contingent pointer so that
       * all of the slots are mentioned by the descriptor.  (The
       * descriptor object itself, from gc_allocated, is in word 0.)
       */
      const weak_gc_ptr<iso_context> _ctxt;
      /*
       * The controlling pointer is the target, and the controlled
       * pointer is the source.  If teh target is gone, there's no
       * point in holding onto the source or going through the
       * roll-up.  (Note that if we have a live <A,B> pair and a <B,C>
       * pair, and B and C are otherwise garbage, the first pair will
       * cause B to be marked, which will mean that the second pair is
       * live and both roll-ups will happen.)
       */
      const contingent_gc_ptr<value_chain, value_chain> _value_chains;
      const weak_gc_ptr<const iso_context::published_state> _pending_state;

      using next_ptr_t = versioned_gc_ptr<pending_rollup, 2>;
      static constexpr next_ptr_t::flag_id<0> this_processed_flag{};
      static constexpr next_ptr_t::flag_id<1> following_processed_flag{};

      /*
       * The flags on _next are for *this* pending_rollup
       */
      next_ptr_t _next;
      


      class process_state;
      friend class process_state;

      bool triage(process_state &pstate);
      bool triage_unknown(process_state &pstate, bool &moved);
      void add_to_published(process_state &);
      void do_rollup();
    public:
      using status_t = iso_context::published_state::status_t;
      pending_rollup(gc_token &gc,
                     const gc_ptr<value_chain> &vc,
                     const gc_ptr<iso_context> &ctxt,
                     const gc_ptr<const iso_context::published_state> &s,
                     const gc_ptr<pending_rollup> &n = nullptr)
        : gc_allocated{gc}, _ctxt{ctxt}, _value_chains{vc->get_parent(), vc}, 
           _pending_state{s}, _next{n}
      {
        // std::cout << "Created pending rollup for " << vc << std::endl;
        // assert(ctxt->is_publishable());
      }
      pending_rollup(gc_token &gc,
                     const gc_ptr<value_chain> &vc,
                     const gc_ptr<const iso_context::published_state> &s,
                     const gc_ptr<pending_rollup> &n = nullptr)
        : pending_rollup{gc, vc, vc->get_context(), s, n}
      {}
      const static auto &descriptor() {
        static gc_descriptor d =
          GC_DESC(pending_rollup)
          .WITH_FIELD(&pending_rollup::_value_chains)
          .WITH_FIELD(&pending_rollup::_ctxt)
          .WITH_FIELD(&pending_rollup::_pending_state)
          .WITH_FIELD(&pending_rollup::_next)
          ;

        return d;
      }

      gc_ptr<pending_rollup> next() const {
        return _next;
      }

      void set_next(const gc_ptr<pending_rollup> &n) {
        _next = n;
      }

      bool processed() const {
        return _next[this_processed_flag];
      }

      void mark_processed() {
        _next[this_processed_flag] = true;
      }

      bool following_processed() const {
        return _next[following_processed_flag];
      }

      void mark_following_processed() {
        _next[following_processed_flag] = true;
      }

      timestamp_t publish_time() const {
        const gc_ptr<const iso_context::published_state> ps = _pending_state.lock();
        assert(ps != nullptr);
        return ps->_timestamp;
      }

      status_t status() {
        const gc_ptr<const iso_context::published_state> ps = _pending_state.lock();
        if (ps == nullptr) {
          /*
           * If the pending state is gone, the publish failed or was
           * abandoned or the context is gone (which means that the
           * target VC is also gone, see comment below).  In any case,
           * there's nothing to do here.
           */
          _next[this_processed_flag] = true;
          return status_t::Failed;
        }
        status_t s = ps->_status;
        if (s == status_t::Pending) {
          gc_ptr<iso_context> c = _ctxt.lock();
          if (c == nullptr) {
            /*
             * If the context is gone, it may not actually have
             * failed, but we know that by now we don't have to worry
             * about it.  If the context is gone, that means that the
             * value chain we'd roll up is gone, and that can only
             * happen if the target VC is also gone, so if the context
             * is gone, we have no place left to roll up to.
             */
            ps->_status = status_t::Failed;
            _next[this_processed_flag] = true;
            return status_t::Failed;
          }
          gc_ptr<const iso_context::state_t> current_state = c->_state;
          if (ps == current_state) {
            s = status_t::Published;
            ps->_status = status_t::Published;
          } else {
            /*
             * We need to check again to guard against the situation
             * in which we weren't the current state when we checked,
             * but between the time we checked our status and the time
             * we read the current state, the publish succeeded
             * (making us Published and the current state) and a
             * conflict was detected, making current_state not us and
             * having a conflict.  In that situation, we should be
             * Published, but we'd read as Failed unless we do the
             * second read.
             *
             * Alternatively, we could read the current_state
             * unconditionally and then read the status.  This would
             * always be right, but would always require two atomic
             * reads (instead of sometimes one and sometimes three).
             * Since we can assume that most of the time the status
             * will resolve reasonably quickly into Pending or Failed,
             * this probably isn't worth it.
             */
            s = ps->_status;
            if (s == status_t::Pending) {
              if (!current_state->conflicts().empty()) {
                s = status_t::Failed;
                ps->_status = status_t::Failed;
                _next[this_processed_flag] = true;
              }
            }
          }
        }
        return s;
      }

      static void process(const gc_ptr<pending_rollup> &prs,
                          std::atomic<gc_ptr<pending_rollup>> &msv_rollups);
      
    }; // pending_rollup

    class msv : public gc_allocated {
      std::atomic<gc_ptr<pending_rollup>> _rollups;

      void process_rollups(const gc_ptr<pending_rollup> &head);
    public:
      using gc_allocated::gc_allocated;

      explicit msv(gc_token &gc)
	: gc_allocated{gc}, _rollups{nullptr}
      {
      }

      const static auto &descriptor() {
        static gc_descriptor d =
          GC_DESC(msv)
          .WITH_FIELD(&msv::_rollups);

        return d;
      }

      void process_rollups() {
        gc_ptr<pending_rollup> prs = _rollups;
        if (prs != nullptr) {
          pending_rollup::process(prs, _rollups);
        }
      }

      void add_rollup(const gc_ptr<value_chain> &vc,
                      const gc_ptr<iso_context> &ctxt,
                      const gc_ptr<const iso_context::published_state> &state)
      {
        gc_ptr<pending_rollup> ru = make_gc<pending_rollup>(vc, ctxt, state);
        ruts::cas_loop(_rollups, [&](const auto &head) {
            ru->set_next(head);
            return ru;
          });
      }

      template <kind K>
      gc_ptr<typed_msv<K>> downcast();
    }; // msv

    class modification : public gc_allocated_with_virtuals<modification, kind> {
      using base = gc_allocated_with_virtuals<modification, kind>;
    public:
      static void init_vf_table(typename base::vf_table &);

      modification(gc_token &gc, discriminator_type d) : base{gc, d} {}
      const static auto &descriptor() {
        static gc_descriptor d =
          GC_DESC(modification)
          .WITH_SUPER(base);
        return d;
      }

      struct virtuals : virtuals_base {
        virtual void perform(modification *self) = 0;
      };

      void perform() {
        call_virtual(this, &virtuals::perform);
      }
    }; // modification
      
    class blocking_mod : public gc_allocated {
      /*
       * If the modification is gone (without being cleared), then
       * either the MSV being modified has been collected or the
       * thread doing the modification died while performing it, and
       * therefore doesn't care whether it's completed or not.
       */
      std::atomic<weak_gc_ptr<modification>> _mod;
    public:
      blocking_mod(gc_token &gc, const gc_ptr<modification> &mod)
        : gc_allocated{gc}, _mod{mod}
      {}

      const static auto &descriptor() {
        static gc_descriptor d =
          GC_DESC(blocking_mod)
          .WITH_FIELD(&blocking_mod::_mod);
        return d;
      }

      bool pending() const {
        return _mod.lock() != nullptr;
      }

      void remove() {
        gc_ptr<modification> m = _mod.lock();
        if (m != nullptr) {
          m->perform();
          /*
           * performing the modification will mark it as done.
           */
        }
      }

      void mark_done() {
        _mod = nullptr;
      }
      
    }; // blocking_mod
      
      
    

    
  }
}

namespace std {
  template <>
  struct hash<mpgc::gc_ptr<mds::core::modified_value_chain>> {
    size_t operator()(const mpgc::gc_ptr<mds::core::modified_value_chain> &mvc) const {
      using namespace std;
      using namespace mds::core;
      return hash<gc_ptr<value_chain>>()(mvc->chain.lock())
        ^ hash<gc_ptr<msv>>()(mvc->in_msv.lock());
    }
  };
}



#endif /* CORE_MSV_H_ */
